{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\nimport datasets\nimport torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:13:57.246394Z","iopub.execute_input":"2023-03-12T18:13:57.246883Z","iopub.status.idle":"2023-03-12T18:14:10.950706Z","shell.execute_reply.started":"2023-03-12T18:13:57.246843Z","shell.execute_reply":"2023-03-12T18:14:10.949509Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:14:10.953346Z","iopub.execute_input":"2023-03-12T18:14:10.954648Z","iopub.status.idle":"2023-03-12T18:14:11.027754Z","shell.execute_reply.started":"2023-03-12T18:14:10.954599Z","shell.execute_reply":"2023-03-12T18:14:11.026578Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_name = \"flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl\"\nai_dataset = datasets.load_dataset(dataset_name, 'ai')['train']\nds_dataset = datasets.load_dataset(dataset_name, 'datascience')['train']\nse_dataset = datasets.load_dataset(dataset_name, 'softwareengineering')['train']","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:14:11.029515Z","iopub.execute_input":"2023-03-12T18:14:11.030264Z","iopub.status.idle":"2023-03-12T18:14:15.410371Z","shell.execute_reply.started":"2023-03-12T18:14:11.030224Z","shell.execute_reply":"2023-03-12T18:14:15.409296Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/14.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb54d79b91ce41239cf6bade3af124e6"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset stack_exchange/ai to /root/.cache/huggingface/datasets/flax-sentence-embeddings___stack_exchange/ai/1.1.0/a767719a162391b61f7fecca12b41572102b8cf2909d9c06f55eb7a70c7aa579...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/146k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0816728167b14c279cb6a093d3910bbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset stack_exchange downloaded and prepared to /root/.cache/huggingface/datasets/flax-sentence-embeddings___stack_exchange/ai/1.1.0/a767719a162391b61f7fecca12b41572102b8cf2909d9c06f55eb7a70c7aa579. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4257240230444baba6f9e6022db4c726"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset stack_exchange/datascience to /root/.cache/huggingface/datasets/flax-sentence-embeddings___stack_exchange/datascience/1.1.0/a767719a162391b61f7fecca12b41572102b8cf2909d9c06f55eb7a70c7aa579...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/261k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c2a6ed66afd450cbeb591dad5cec568"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset stack_exchange downloaded and prepared to /root/.cache/huggingface/datasets/flax-sentence-embeddings___stack_exchange/datascience/1.1.0/a767719a162391b61f7fecca12b41572102b8cf2909d9c06f55eb7a70c7aa579. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3025854c1ca94ead937f4958e6d7d5fc"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset stack_exchange/softwareengineering to /root/.cache/huggingface/datasets/flax-sentence-embeddings___stack_exchange/softwareengineering/1.1.0/a767719a162391b61f7fecca12b41572102b8cf2909d9c06f55eb7a70c7aa579...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.01M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88b04a0ce51d42d387e246b36b407af0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset stack_exchange downloaded and prepared to /root/.cache/huggingface/datasets/flax-sentence-embeddings___stack_exchange/softwareengineering/1.1.0/a767719a162391b61f7fecca12b41572102b8cf2909d9c06f55eb7a70c7aa579. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c478b557790c440284192080085b3550"}},"metadata":{}}]},{"cell_type":"code","source":"combined_dataset = datasets.concatenate_datasets([ai_dataset, ds_dataset, se_dataset])\n# Change column names\ncombined_dataset = combined_dataset.rename_column(\"title_body\", \"question\")\ncombined_dataset = combined_dataset.rename_column(\"upvoted_answer\", \"answer\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:14:15.413232Z","iopub.execute_input":"2023-03-12T18:14:15.413981Z","iopub.status.idle":"2023-03-12T18:14:15.428850Z","shell.execute_reply.started":"2023-03-12T18:14:15.413935Z","shell.execute_reply":"2023-03-12T18:14:15.427750Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"del ai_dataset, ds_dataset, se_dataset","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:14:15.430500Z","iopub.execute_input":"2023-03-12T18:14:15.431145Z","iopub.status.idle":"2023-03-12T18:14:15.437374Z","shell.execute_reply.started":"2023-03-12T18:14:15.431107Z","shell.execute_reply":"2023-03-12T18:14:15.436212Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Fine-tune T5 on the dataset\nmodel_name = \"t5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:14:15.438939Z","iopub.execute_input":"2023-03-12T18:14:15.439610Z","iopub.status.idle":"2023-03-12T18:14:25.799838Z","shell.execute_reply.started":"2023-03-12T18:14:15.439574Z","shell.execute_reply":"2023-03-12T18:14:25.798799Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a8b5020a5534a8e9293939a458ed370"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed2b9452a8b54b7e82e97017147623f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dbdacc9ae3e48dea2128e841c0408fb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5_fast.py:165: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9517d485ab648d7a1c988f42d6f70a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"745e31396e9a4543b7a3ceee0c960ab1"}},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = [f\"question: {q} context and answer: {a}\" for q, a in zip(examples[\"question\"], examples[\"answer\"])]\n    model_inputs = tokenizer(inputs, max_length=512, padding=\"max_length\", truncation=True)\n\n    # Setup the tokenizer for targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(examples[\"question\"], max_length=512, padding=\"max_length\", truncation=True)\n\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:14:25.801341Z","iopub.execute_input":"2023-03-12T18:14:25.802582Z","iopub.status.idle":"2023-03-12T18:14:25.810216Z","shell.execute_reply.started":"2023-03-12T18:14:25.802541Z","shell.execute_reply":"2023-03-12T18:14:25.809207Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = combined_dataset.map(preprocess_function, batched=True, num_proc=4)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:14:25.812061Z","iopub.execute_input":"2023-03-12T18:14:25.812970Z","iopub.status.idle":"2023-03-12T18:14:38.938688Z","shell.execute_reply.started":"2023-03-12T18:14:25.812930Z","shell.execute_reply":"2023-03-12T18:14:38.937312Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"        ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44f8e1ac55de4274b136d911f6b12bf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67d71fa202e846a1bdb256ab117dc196"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd8f3997fd654fca858004833c19375c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02f3b484a4e14dbfb79be50399403687"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:3582: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  \"`as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your \"\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:3582: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  \"`as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your \"\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:3582: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  \"`as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your \"\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:3582: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  \"`as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your \"\n","output_type":"stream"}]},{"cell_type":"code","source":"def split_train_validation(examples):\n    train_size = int(0.9 * len(examples))\n    return {\"train\": examples.select(range(train_size)), \"validation\": examples.select(range(train_size, len(examples)))}","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:14:38.940706Z","iopub.execute_input":"2023-03-12T18:14:38.941108Z","iopub.status.idle":"2023-03-12T18:14:38.951110Z","shell.execute_reply.started":"2023-03-12T18:14:38.941059Z","shell.execute_reply":"2023-03-12T18:14:38.950145Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"split = split_train_validation(tokenized_datasets)\nprint(len(split['train']))\nprint(len(split['validation']))","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:14:38.956005Z","iopub.execute_input":"2023-03-12T18:14:38.956493Z","iopub.status.idle":"2023-03-12T18:14:39.076980Z","shell.execute_reply.started":"2023-03-12T18:14:38.956449Z","shell.execute_reply":"2023-03-12T18:14:39.075808Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"4223\n470\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    \"test-t5\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    weight_decay=0.01,\n    num_train_epochs=1,\n    predict_with_generate=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:14:39.079199Z","iopub.execute_input":"2023-03-12T18:14:39.080447Z","iopub.status.idle":"2023-03-12T18:14:39.101340Z","shell.execute_reply.started":"2023-03-12T18:14:39.080415Z","shell.execute_reply":"2023-03-12T18:14:39.100447Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:14:39.104023Z","iopub.execute_input":"2023-03-12T18:14:39.104354Z","iopub.status.idle":"2023-03-12T18:14:43.867910Z","shell.execute_reply.started":"2023-03-12T18:14:39.104319Z","shell.execute_reply":"2023-03-12T18:14:43.866860Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model,\n    training_args,\n    train_dataset=split[\"train\"],\n    eval_dataset=split[\"validation\"],\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:14:43.869326Z","iopub.execute_input":"2023-03-12T18:14:43.870371Z","iopub.status.idle":"2023-03-12T18:14:43.883117Z","shell.execute_reply.started":"2023-03-12T18:14:43.870328Z","shell.execute_reply":"2023-03-12T18:14:43.882159Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:14:43.884595Z","iopub.execute_input":"2023-03-12T18:14:43.885125Z","iopub.status.idle":"2023-03-12T18:28:09.896742Z","shell.execute_reply.started":"2023-03-12T18:14:43.885080Z","shell.execute_reply":"2023-03-12T18:28:09.892748Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: downvoted_answer, answer, question. If downvoted_answer, answer, question are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 4223\n  Num Epochs = 1\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1056\n  Number of trainable parameters = 222903552\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.11 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.13.10"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230312_181540-mmp5lt3l</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bgayther/huggingface/runs/mmp5lt3l' target=\"_blank\">generous-wildflower-2</a></strong> to <a href='https://wandb.ai/bgayther/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bgayther/huggingface' target=\"_blank\">https://wandb.ai/bgayther/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bgayther/huggingface/runs/mmp5lt3l' target=\"_blank\">https://wandb.ai/bgayther/huggingface/runs/mmp5lt3l</a>"},"metadata":{}},{"name":"stderr","text":"You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1056' max='1056' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1056/1056 11:55, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.031300</td>\n      <td>0.005845</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to test-t5/checkpoint-500\nConfiguration saved in test-t5/checkpoint-500/config.json\nConfiguration saved in test-t5/checkpoint-500/generation_config.json\nModel weights saved in test-t5/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in test-t5/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in test-t5/checkpoint-500/special_tokens_map.json\nCopy vocab file to test-t5/checkpoint-500/spiece.model\nSaving model checkpoint to test-t5/checkpoint-1000\nConfiguration saved in test-t5/checkpoint-1000/config.json\nConfiguration saved in test-t5/checkpoint-1000/generation_config.json\nModel weights saved in test-t5/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in test-t5/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in test-t5/checkpoint-1000/special_tokens_map.json\nCopy vocab file to test-t5/checkpoint-1000/spiece.model\nThe following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: downvoted_answer, answer, question. If downvoted_answer, answer, question are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 470\n  Batch size = 4\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1056, training_loss=0.4252825099410433, metrics={'train_runtime': 805.9463, 'train_samples_per_second': 5.24, 'train_steps_per_second': 1.31, 'total_flos': 2571629171834880.0, 'train_loss': 0.4252825099410433, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}